import re
from typing import List, Tuple, Optional

class Lexer:
    """
    è¯æ³•åˆ†æå™¨ - ä¿®å¤åˆ—è¡¨é¡¹å†…å®¹æå–é—®é¢˜
    """
    
    # å®šä¹‰è¯æ³•è§„åˆ™ï¼ˆæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼‰- ä¿®å¤å‰å¯¼ç©ºç™½ç¬¦æ•æ„Ÿæ€§é—®é¢˜
    PATTERNS = {
        'archive': re.compile(r'^\s*--<([^>]+)>--\s*$'),  # ä¿®å¤ï¼šå…è®¸å‰å¯¼ç©ºç™½ç¬¦
        'card_start': re.compile(r'^\s*<\+([^/]+)\s*$'),   # ä¿®å¤ï¼šå…è®¸å‰å¯¼ç©ºç™½ç¬¦
        'card_end': re.compile(r'^\s*/\+>\s*$'),          # ä¿®å¤ï¼šå…è®¸å‰å¯¼ç©ºç™½ç¬¦
        'label': re.compile(r'\(([^:]+):\s*([^)]+)\)'),
        'region': re.compile(r'^\s*---\s*([^\s].*?)\s*$'), # ä¿®å¤ï¼šå…è®¸å‰å¯¼ç©ºç™½ç¬¦
        'vibe_card': re.compile(r'`([^`\n]+)`\[([^\]]*)\]'),
        'inline_explanation': re.compile(r'>>\s*([^\n]+?)\s*$'),
        'code_block_start': re.compile(r'^\s*```\s*(\w*)\s*$'),  # ä¿®å¤ï¼šå…è®¸å‰å¯¼ç©ºç™½ç¬¦
        'code_block_end': re.compile(r'^\s*```\s*$'),           # ä¿®å¤ï¼šå…è®¸å‰å¯¼ç©ºç™½ç¬¦
        'ordered_list': re.compile(r'^\s*(\d+\.\s+.*)$'),       # ä¿®å¤ï¼šå…è®¸å‰å¯¼ç©ºç™½ç¬¦
        'unordered_list': re.compile(r'^\s*(-\s+.*)$'),         # ä¿®å¤ï¼šå…è®¸å‰å¯¼ç©ºç™½ç¬¦ï¼Œçº æ­£æ¨¡å¼é”™è¯¯
        'bold_text': re.compile(r'\*\*(.*?)\*\*'),
        'empty_line': re.compile(r'^\s*$')
    }
    
    def __init__(self):
        self.tokens = []
        self.current_line_num = 0
        self.in_code_block = False
        self.current_code_block_lang = ""
        self.current_code_block_content = []
    
    def tokenize(self, text: str) -> List[Tuple[str, str, int]]:
        """å°†è¾“å…¥æ–‡æœ¬åˆ†è§£ä¸ºè¯æ³•æ ‡è®°"""
        self.tokens = []
        self.current_line_num = 0
        self.in_code_block = False
        lines = text.split('\n')
        
        for line in lines:
            self.current_line_num += 1
            self._process_line(line)
        
        return self.tokens
    
    def _process_line(self, line: str):
        """å¤„ç†å•è¡Œæ–‡æœ¬ - å¢å¼ºç©ºç™½ç¬¦å®¹é”™èƒ½åŠ›"""
        # å¤„ç†ä»£ç å—çŠ¶æ€
        if self.in_code_block:
            if self.PATTERNS['code_block_end'].match(line.strip()):  # ä½¿ç”¨strip()ç¡®ä¿åŒ¹é…
                # ç»“æŸä»£ç å—
                if self.current_code_block_content:
                    code_content = '\n'.join(self.current_code_block_content)
                    self.tokens.append(('CODE_BLOCK_CONTENT', code_content, self.current_line_num - len(self.current_code_block_content)))
                
                self.tokens.append(('CODE_BLOCK_END', '', self.current_line_num))
                self.in_code_block = False
                self.current_code_block_content = []
            else:
                # åœ¨ä»£ç å—ä¸­ï¼Œæ”¶é›†å†…å®¹
                self.current_code_block_content.append(line)
            return
        
        # æ£€æŸ¥ç©ºè¡Œ
        if self.PATTERNS['empty_line'].match(line):
            self.tokens.append(('EMPTY_LINE', '', self.current_line_num))
            return
        
        # æ£€æŸ¥ä»£ç å—å¼€å§‹
        code_start_match = self.PATTERNS['code_block_start'].match(line)
        if code_start_match:
            self.tokens.append(('CODE_BLOCK_START', code_start_match.group(1), self.current_line_num))
            self.in_code_block = True
            self.current_code_block_lang = code_start_match.group(1)
            self.current_code_block_content = []
            return
        
        # æ£€æŸ¥å½’æ¡£å®šä¹‰ - ä½¿ç”¨ä¿®å¤åçš„æ¨¡å¼ï¼ˆç°åœ¨å…è®¸å‰å¯¼ç©ºç™½ï¼‰
        archive_match = self.PATTERNS['archive'].match(line)
        if archive_match:
            self.tokens.append(('ARCHIVE', archive_match.group(1).strip(), self.current_line_num))
            return
        
        # æ£€æŸ¥å¡ç‰‡å¼€å§‹ - ä½¿ç”¨ä¿®å¤åçš„æ¨¡å¼ï¼ˆç°åœ¨å…è®¸å‰å¯¼ç©ºç™½ï¼‰
        card_start_match = self.PATTERNS['card_start'].match(line)
        if card_start_match:
            self.tokens.append(('CARD_START', card_start_match.group(1).strip(), self.current_line_num))
            return
        
        # æ£€æŸ¥å¡ç‰‡ç»“æŸ - ä½¿ç”¨ä¿®å¤åçš„æ¨¡å¼ï¼ˆç°åœ¨å…è®¸å‰å¯¼ç©ºç™½ï¼‰
        card_end_match = self.PATTERNS['card_end'].match(line)
        if card_end_match:
            self.tokens.append(('CARD_END', '', self.current_line_num))
            return
        
        # æ£€æŸ¥åŒºåŸŸå®šä¹‰ - ä½¿ç”¨ä¿®å¤åçš„æ¨¡å¼ï¼ˆç°åœ¨å…è®¸å‰å¯¼ç©ºç™½ï¼‰
        region_match = self.PATTERNS['region'].match(line)
        if region_match:
            region_name = region_match.group(1).strip() if region_match.group(1) else ""
            self.tokens.append(('REGION', region_name, self.current_line_num))
            return
        
        # å¤„ç†è¡Œå†…å…ƒç´  - ä¿æŒåŸæœ‰é€»è¾‘
        self._process_inline_elements(line)
    
    def _process_inline_elements(self, line: str):
        """å¤„ç†è¡Œå†…çš„å„ç§å…ƒç´  - ä¿®å¤åˆ—è¡¨é¡¹ä¸­çš„å†…è”å…ƒç´ """
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯æ ‡ç­¾ï¼ˆæ ‡ç­¾é€šå¸¸ç‹¬ç«‹æˆè¡Œï¼‰
        label_match = self.PATTERNS['label'].match(line)
        if label_match:
            symbol = label_match.group(1).strip()
            content = [c.strip() for c in label_match.group(2).split(',')]
            self.tokens.append(('LABEL', f"{symbol}:{','.join(content)}", self.current_line_num))
            return
        
        # æ£€æŸ¥åˆ—è¡¨é¡¹ï¼ˆæœ‰åºå’Œæ— åºï¼‰
        ordered_match = self.PATTERNS['ordered_list'].match(line)
        unordered_match = self.PATTERNS['unordered_list'].match(line)
        
        if ordered_match or unordered_match:
            # æå–å®Œæ•´çš„åˆ—è¡¨é¡¹å†…å®¹ï¼ˆåŒ…æ‹¬æ ‡è®°ï¼‰
            list_content = line.strip()
            
            print(f"ğŸ” LEXER: åˆ—è¡¨é¡¹å†…å®¹='{list_content}'")
            
            # æ ‡è®°åˆ—è¡¨é¡¹å¼€å§‹
            list_type = 'ORDERED_LIST_ITEM' if ordered_match else 'UNORDERED_LIST_ITEM'
            self.tokens.append((list_type, list_content, self.current_line_num))
            
            # å¤„ç†åˆ—è¡¨é¡¹å†…å®¹ä¸­çš„è¡Œå†…å…ƒç´ 
            # ä¿®å¤ï¼šæå–å†…å®¹éƒ¨åˆ†ï¼ˆå»æ‰åˆ—è¡¨æ ‡è®°ï¼‰
            if ordered_match:
                content = ordered_match.group(1).strip()
            else:
                content = unordered_match.group(1).strip()
                
            self._process_line_content(content)
            return
        
        # å¤„ç†æ™®é€šè¡Œå†…å®¹
        self._process_line_content(line)

    def _process_line_content(self, content: str):
        """å¤„ç†è¡Œå†…å®¹ä¸­çš„å„ç§è¡Œå†…å…ƒç´ """
        remaining = content.strip()
        
        while remaining:
            # 1. æ£€æŸ¥æ³¨å¡
            vibe_card_match = self.PATTERNS['vibe_card'].search(remaining)
            if vibe_card_match:
                # æ·»åŠ æ³¨å¡å‰çš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
                before_text = remaining[:vibe_card_match.start()].strip()
                if before_text:
                    self.tokens.append(('TEXT', before_text, self.current_line_num))
                
                # æ·»åŠ æ³¨å¡
                card_content = vibe_card_match.group(1)
                annotation = vibe_card_match.group(2)
                self.tokens.append(('VIBE_CARD', f"{card_content}[{annotation}]", self.current_line_num))
                
                # æ›´æ–°å‰©ä½™å†…å®¹
                remaining = remaining[vibe_card_match.end():].strip()
                continue
            
            # 2. æ£€æŸ¥è¡Œå†…è§£é‡Š
            inline_exp_match = self.PATTERNS['inline_explanation'].search(remaining)
            if inline_exp_match:
                # æ·»åŠ è¡Œå†…è§£é‡Šå‰çš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
                before_text = remaining[:inline_exp_match.start()].strip()
                if before_text:
                    self.tokens.append(('TEXT', before_text, self.current_line_num))
                
                # æ·»åŠ è¡Œå†…è§£é‡Š
                explanation = inline_exp_match.group(1)
                self.tokens.append(('INLINE_EXPLANATION', explanation, self.current_line_num))
                
                # æ›´æ–°å‰©ä½™å†…å®¹
                remaining = remaining[inline_exp_match.end():].strip()
                continue
            
            # 3. æ£€æŸ¥åŠ ç²—æ–‡æœ¬
            bold_match = self.PATTERNS['bold_text'].search(remaining)
            if bold_match:
                # æ·»åŠ åŠ ç²—æ–‡æœ¬å‰çš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
                before_text = remaining[:bold_match.start()].strip()
                if before_text:
                    self.tokens.append(('TEXT', before_text, self.current_line_num))
                
                # æ·»åŠ åŠ ç²—æ–‡æœ¬
                bold_content = bold_match.group(1)
                self.tokens.append(('BOLD_TEXT', bold_content, self.current_line_num))
                
                # æ›´æ–°å‰©ä½™å†…å®¹
                remaining = remaining[bold_match.end():].strip()
                continue
            
            # 4. å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•ç‰¹æ®Šæ¨¡å¼ï¼Œå°†å‰©ä½™å†…å®¹ä½œä¸ºæ™®é€šæ–‡æœ¬
            if remaining:
                self.tokens.append(('TEXT', remaining, self.current_line_num))
                break

def print_tokens(tokens):
    """æ‰“å°è¯æ³•åˆ†æç»“æœ"""
    print("è¯æ³•åˆ†æç»“æœ:")
    print("-" * 50)
    for token_type, token_value, line_num in tokens:
        print(f"è¡Œ {line_num:3d}: {token_type:20} {token_value}")
    print("-" * 50)

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # æµ‹è¯•ç”¨ä¾‹
    test_code = """
--<Vocabulary>--
<+vigilant
(>: #careful, #watchful, è¿‘ä¹‰è¯)
(!: é‡è¦)
---è§£é‡Š
adj. è­¦æƒ•çš„ï¼›è­¦è§‰çš„ï¼›æˆ’å¤‡çš„
---è¯ç»„
- be vigilant about/against/over >>å¯¹â€¦ä¿æŒè­¦æƒ•
- remain/stay vigilant >>ä¿æŒè­¦æƒ•
- require vigilance >>ï¼ˆéœ€è¦è­¦æƒ•æ€§ï¼‰
---ä¾‹å¥
- Citizens are urged to remain vigilant against cyber scams. `ç½‘ç»œè¯ˆéª—`[æŒ‡é€šè¿‡äº’è”ç½‘è¿›è¡Œçš„æ¬ºè¯ˆè¡Œä¸º] >>æ•¦ä¿ƒå…¬æ°‘å¯¹ç½‘ç»œè¯ˆéª—ä¿æŒè­¦æƒ•
/+>
"""
    lexer = Lexer()
    tokens = lexer.tokenize(test_code)
    print_tokens(tokens)